kuberay-operator:
 enabled: false

name: llm-engine
rayVersion: 2.49.2
image: "rayproject/ray-llm:{{ .Values.rayVersion }}-py311-cu128"
models:
  - "Qwen/Qwen2.5-7B-Instruct"
  - "unsloth/Meta-Llama-3.1-8B-Instruct"
working_dir: https://github.com/kaasops/llm-engine/archive/refs/heads/master.zip
import_path: engine:app
hf_token: ''
ray_actor_options:
  num_cpus: 4.0
  num_gpus: 1.0

extraApplications: []
  # - import_path: ray.serve.llm:build_openai_app
  #   name: buildin_vllm_engine
  #   route_prefix: /
  #   args:
  #     llm_configs:
  #       - model_loading_config:
  #           model_id: "qwen-0.5b"
  #           model_source: "Qwen/Qwen2.5-0.5B-Instruct"

headGroupSpec:
  rayStartParams:
    num-cpus: "1"
    num-gpus: "0"
  resources:
    limits:
      cpu: "2"
      memory: "8Gi"
    requests:
      cpu: "1"
      memory: "4Gi"
workerGroupSpec:
  replicas: 1
  minReplicas: 1
  maxReplicas: 2
  rayStartParams:
    num-cpus: "8"
    num-gpus: "1"
  resources:
    limits:
      cpu: "8"
      memory: "64Gi"
      nvidia.com/gpu: 1
    requests:
      cpu: "8"
      memory: "32Gi"
      nvidia.com/gpu: 1

ingress:
  enabled: false
  host: "llm-engine.local"
  # ingressClassName: "nginx"  # Uncomment and set if using specific ingress controller
  # tls:
  #   - hosts:
  #       - "llm-engine.example.com"
  #     secretName: llm-engine-tls
  # annotations:
  #   kubernetes.io/ingress.class: "nginx"
  #   nginx.ingress.kubernetes.io/rewrite-target: /
  #   nginx.ingress.kubernetes.io/ssl-redirect: "false"