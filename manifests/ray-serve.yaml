apiVersion: ray.io/v1
kind: RayService
metadata:
  name: llm-engine
spec:
  # RayCluster configuration
  rayClusterConfig:
    rayVersion: '2.49.2'
    enableInTreeAutoscaling: true
    autoscalerOptions:
      version: v2
      upscalingMode: Default
      idleTimeoutSeconds: 60
      imagePullPolicy: IfNotPresent
      resources:
        limits:
          cpu: "500m"
          memory: "512Mi"
        requests:
          cpu: "500m"
          memory: "512Mi"
    # Head group configuration
    headGroupSpec:
      rayStartParams:
        dashboard-host: '0.0.0.0'
        num-cpus: "1"
        num-gpus: "0"
      template:
        spec:
          containers:
          - name: ray-head
            image: rayproject/ray-llm:2.49.2-py311-cu128
            ports:
            - containerPort: 6379
              name: gcs
            - containerPort: 8265
              name: dashboard
            - containerPort: 10001
              name: client
            - containerPort: 8000
              name: serving
            resources:
              limits:
                cpu: "2"
                memory: "8Gi"
              requests:
                cpu: "1"
                memory: "4Gi"
    # Worker group configuration
    workerGroupSpecs:
    - replicas: 1
      minReplicas: 1
      maxReplicas: 2
      groupName: gpu-worker
      rayStartParams:
        num-cpus: "8"
        num-gpus: "1"
      template:
        spec:
          containers:
          - name: ray-worker
            image: rayproject/ray-llm:2.49.2-py311-cu128
            resources:
              limits:
                cpu: "8"
                memory: "64Gi"
                nvidia.com/gpu: 1
              requests:
                cpu: "8"
                memory: "32Gi"
                nvidia.com/gpu: 1
  serveConfigV2: |
    applications:
      - name: vLLM_engine
        import_path: engine:app
        route_prefix: /
        runtime_env:
          working_dir: "https://github.com/kaasops/llm-engine/archive/refs/heads/master.zip"
          env_vars:
            VLLM_USE_V1: "1"
            HF_TOKEN: ""
        deployments:
          - name: LLMServingAPI
            user_config:
              models:
                - model_id: Qwen2.5-7B-Instruct
                  model_source: Qwen/Qwen2.5-7B-Instruct
            ray_actor_options:
              num_cpus: 4.0
              num_gpus: 1.0
---
